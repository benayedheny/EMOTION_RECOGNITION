{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import os\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawings = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "FACEMESH_LIPS = [(61, 146), (146, 91), (91, 181), (181, 84), (84, 17),\n",
    "                 (17, 314), (314, 405), (405, 321), (321, 375),\n",
    "                 (375, 291), (61, 185), (185, 40), (40, 39), (39, 37),\n",
    "                 (37, 0), (0, 267),\n",
    "                 (267, 269), (269, 270), (270, 409), (409, 291),\n",
    "                 (78, 95), (95, 88), (88, 178), (178, 87), (87, 14),\n",
    "                 (14, 317), (317, 402), (402, 318), (318, 324),\n",
    "                 (324, 308), (78, 191), (191, 80), (80, 81), (81, 82),\n",
    "                 (82, 13), (13, 312), (312, 311), (311, 310),\n",
    "                 (310, 415), (415, 308)]\n",
    "\n",
    "FACEMESH_LEFT_EYE = [(263, 249), (249, 390), (390, 373), (373, 374),\n",
    "                     (374, 380), (380, 381), (381, 382), (382, 362),\n",
    "                     (263, 466), (466, 388), (388, 387), (387, 386),\n",
    "                     (386, 385), (385, 384), (384, 398), (398, 362)]\n",
    "\n",
    "FACEMESH_LEFT_IRIS = [(474, 475), (475, 476), (476, 477),\n",
    "                      (477, 474)]\n",
    "\n",
    "FACEMESH_LEFT_EYEBROW = [(276, 283), (283, 282), (282, 295),\n",
    "                         (295, 285), (300, 293), (293, 334),\n",
    "                         (334, 296), (296, 336)]\n",
    "\n",
    "FACEMESH_RIGHT_EYE = [(33, 7), (7, 163), (163, 144), (144, 145),\n",
    "                      (145, 153), (153, 154), (154, 155), (155, 133),\n",
    "                      (33, 246), (246, 161), (161, 160), (160, 159),\n",
    "                      (159, 158), (158, 157), (157, 173), (173, 133)]\n",
    "\n",
    "FACEMESH_RIGHT_EYEBROW = [(46, 53), (53, 52), (52, 65), (65, 55),\n",
    "                          (70, 63), (63, 105), (105, 66), (66, 107)]\n",
    "\n",
    "FACEMESH_RIGHT_IRIS = [(469, 470), (470, 471), (471, 472),\n",
    "                       (472, 469)]\n",
    "\n",
    "TrainingDataDirectory = \"../DATA/Train/\"\n",
    "TestingDataDirectory = \"../DATA/Test/\"\n",
    "Classes = [x for x in os.listdir(TrainingDataDirectory)]\n",
    "UPPER_REF = 10\n",
    "LOWER_REF = 152"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def FACEMESH_COMPRESSOR(FACEMESH, n):\n",
    "    TEMP_FACEMESH = []\n",
    "    FINAL_FACEMESH = []\n",
    "    for i in range(len(FACEMESH)):\n",
    "        TEMP_FACEMESH.append(FACEMESH[i][0])\n",
    "        TEMP_FACEMESH.append(FACEMESH[i][1])\n",
    "    TEMP_FACEMESH = list(dict.fromkeys(TEMP_FACEMESH))\n",
    "    for i in range(0, len(TEMP_FACEMESH), n):\n",
    "        FINAL_FACEMESH.append(TEMP_FACEMESH[i])\n",
    "    return FINAL_FACEMESH"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "FOREHEAD_TO_LEFT_EYEBROW = [(UPPER_REF,46),(UPPER_REF,63),(UPPER_REF,52),(UPPER_REF,66),(UPPER_REF,55)]\n",
    "FOREHEAD_TO_RIGHT_EYEBROW = [(UPPER_REF,285),(UPPER_REF,296),(UPPER_REF,282),(UPPER_REF,293),(UPPER_REF,276)]\n",
    "LEFT_EYE_TO_LEFT_EYEBROW = [(33,46),(161,63),(159,52),(157,66),(133,55)]\n",
    "RIGHT_EYE_TO_RIGHT_EYEBROW = [(362,285),(384,296),(386,282),(388,293),(263,276)]\n",
    "LEFT_EYEBROW_TO_RIGHT_EYEBROW = [(55,285)]\n",
    "LEFT_EYE_HEIGHT = [(161,163),(159,145),(157,154)]\n",
    "LEFT_EYE_WIDTH = [(33,133)]\n",
    "RIGHT_EYE_HEIGHT = [(384,381),(386,374),(388,390)]\n",
    "RIGHT_EYE_WIDTH = [(362,263)]\n",
    "MOUTH_LEFT_CORNER_TO_LOWER_LEFT_EYE = [(61,33),(61,163),(61,145),(61,154),(61,133)]\n",
    "MOUTH_RIGHT_CORNER_TO_LOWER_RIGHT_EYE = [(291,362),(291,381),(291,374),(291,390),(291,263)]\n",
    "MOUTH_HEIGHT = [(40,91),(37,84),(314,267),(321,270)]\n",
    "MOUTH_WIDTH = [(61,291)]\n",
    "CHIN_TO_LOWER_LIP = [(LOWER_REF,61),(LOWER_REF,91),(LOWER_REF,84),(LOWER_REF,314),(LOWER_REF,321),(LOWER_REF,291)]\n",
    "LOWER_NOSE_TO_UPPER_LIP = [(1,291),(1,40),(1,37),(1,267),(1,270)]\n",
    "NOSE_HEIGHT=[(1,6)]\n",
    "UPPER_NOSE_TO_INNER_EYES = [(6,133),(6,362)]\n",
    "UPPER_NOSE_TO_INNER_EYEBROWS = [(6,55),(6,285)]\n",
    "\n",
    "DISTANCES = FOREHEAD_TO_LEFT_EYEBROW+FOREHEAD_TO_RIGHT_EYEBROW+LEFT_EYE_TO_LEFT_EYEBROW+RIGHT_EYE_TO_RIGHT_EYEBROW+LEFT_EYEBROW_TO_RIGHT_EYEBROW+LEFT_EYE_HEIGHT+LEFT_EYE_WIDTH+RIGHT_EYE_HEIGHT+RIGHT_EYE_WIDTH+MOUTH_LEFT_CORNER_TO_LOWER_LEFT_EYE+MOUTH_RIGHT_CORNER_TO_LOWER_RIGHT_EYE+MOUTH_HEIGHT+MOUTH_WIDTH+CHIN_TO_LOWER_LIP+LOWER_NOSE_TO_UPPER_LIP+NOSE_HEIGHT+UPPER_NOSE_TO_INNER_EYES+UPPER_NOSE_TO_INNER_EYEBROWS\n",
    "\n",
    "print(len(DISTANCES))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def calculate_distance(a,b):\n",
    "    return sqrt((a.x-b.x)*(a.x-b.x) + (a.y-b.y)*(a.y-b.y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CAM WITH LANDMARKS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# drawing_spec = mp_drawings.DrawingSpec(thickness=1, circle_radius=1)\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# with mp_face_mesh.FaceMesh(\n",
    "#     max_num_faces=1,\n",
    "#     refine_landmarks=True,\n",
    "#     min_detection_confidence=0.5,\n",
    "#     min_tracking_confidence=0.5) as face_mesh:\n",
    "#   while cap.isOpened():\n",
    "#     success, annotated_image = cap.read()\n",
    "#     height,width,_= annotated_image.shape\n",
    "#     if not success:\n",
    "#       print(\"Ignoring empty camera frame.\")\n",
    "#       # If loading a video, use 'break' instead of 'continue'.\n",
    "#       continue\n",
    "#\n",
    "#     # To improve performance, optionally mark the image as not writeable to\n",
    "#     # pass by reference.\n",
    "#     annotated_image.flags.writeable = False\n",
    "#     annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "#     results = face_mesh.process(annotated_image)\n",
    "#\n",
    "#     # Draw the face mesh annotations on the image.\n",
    "#     annotated_image.flags.writeable = True\n",
    "#     annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "#     if results.multi_face_landmarks:\n",
    "#       for face_landmarks in results.multi_face_landmarks:\n",
    "#           for i in landmarks:\n",
    "#               x=int(face_landmarks.landmark[i].x * width)\n",
    "#               y=int(face_landmarks.landmark[i].y * height)\n",
    "#               cv2.circle(img=annotated_image,center=(x,y),radius=2,color=(0,0,255),thickness=-1)\n",
    "#               cv2.putText(img=annotated_image,text=str(i),org=(x,y+3),fontFace=cv2.FONT_HERSHEY_DUPLEX,fontScale=0.3,color=(255,0,0),thickness=1,)\n",
    "#           for i in distances:\n",
    "#               x1=int(face_landmarks.landmark[i[0]].x * width)\n",
    "#               y1=int(face_landmarks.landmark[i[0]].y * height)\n",
    "#               x2=int(face_landmarks.landmark[i[1]].x * width)\n",
    "#               y2=int(face_landmarks.landmark[i[1]].y * height)\n",
    "#               cv2.line(img=annotated_image,pt1=(x1,y1),pt2=(x2,y2),color=(255,0,0),thickness=2)\n",
    "#     # Flip the image horizontally for a selfie-view display.\n",
    "#     cv2.imshow('MediaPipe Face Mesh', cv2.flip(annotated_image, 1))\n",
    "#     if cv2.waitKey(5) & 0xFF == 27:\n",
    "#         cv2.destroyAllWindows()\n",
    "#         break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "#\n",
    "# loaded_model = joblib.load(\"./fer2013_random_forest.joblib\")\n",
    "# drawing_spec = mp_drawings.DrawingSpec(thickness=1, circle_radius=1)\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# data=[]\n",
    "# emotions = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Neutral', 5:'Sad', 6:'Surprise'}\n",
    "# with mp_face_mesh.FaceMesh(\n",
    "#         max_num_faces=1,\n",
    "#         refine_landmarks=True,\n",
    "#         min_detection_confidence=0.5,\n",
    "#         min_tracking_confidence=0.5) as face_mesh:\n",
    "#     while cap.isOpened():\n",
    "#         success, annotated_image = cap.read()\n",
    "#         height,width,_= annotated_image.shape\n",
    "#         if not success:\n",
    "#             print(\"Ignoring empty camera frame.\")\n",
    "#             # If loading a video, use 'break' instead of 'continue'.\n",
    "#             continue\n",
    "#\n",
    "#         # To improve performance, optionally mark the image as not writeable to\n",
    "#         # pass by reference.\n",
    "#         annotated_image.flags.writeable = False\n",
    "#         annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "#         results = face_mesh.process(annotated_image)\n",
    "#\n",
    "#         # Draw the face mesh annotations on the image.\n",
    "#         annotated_image.flags.writeable = True\n",
    "#         annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "#         if results.multi_face_landmarks:\n",
    "#             for face_landmarks in results.multi_face_landmarks:\n",
    "#                 data = []\n",
    "#                 for i in distances:\n",
    "#                     x1=int(face_landmarks.landmark[i[0]].x * width)\n",
    "#                     y1=int(face_landmarks.landmark[i[0]].y * height)\n",
    "#                     x2=int(face_landmarks.landmark[i[1]].x * width)\n",
    "#                     y2=int(face_landmarks.landmark[i[1]].y * height)\n",
    "#                     data.append(calculate_distance(face_landmarks.landmark[i[0]],face_landmarks.landmark[i[1]],height))\n",
    "#                     time.sleep(3)\n",
    "#             print(data)\n",
    "#             if loaded_model.predict(np.array(data).reshape((1, -1)))[0].tolist().__contains__(1):\n",
    "#                 cv2.putText(img=annotated_image,text=emotions[loaded_model.predict(np.array(data).reshape((1, -1)))[0].tolist().index(1)],org=(100,100),fontFace=cv2.FONT_HERSHEY_DUPLEX,fontScale=0.3,color=(255,0,0),thickness=1,)\n",
    "#             else:\n",
    "#                 cv2.putText(img=annotated_image,text='PLEASE WAIT',org=(100,100),fontFace=cv2.FONT_HERSHEY_DUPLEX,fontScale=0.3,color=(255,0,0),thickness=1,)\n",
    "#         # Flip the image horizontally for a selfie-view display.\n",
    "#         cv2.imshow('MediaPipe Face Mesh', cv2.flip(annotated_image, 1))\n",
    "#         if cv2.waitKey(5) & 0xFF == 27:\n",
    "#             cv2.destroyAllWindows()\n",
    "#             break\n",
    "# cap.release()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "data = []\n",
    "loaded_model = joblib.load(\"./fer2013_model.joblib\")\n",
    "# emotions = {0:'NO', 1:'YES'}\n",
    "emotions = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Neutral', 5:'Sad', 6:'Surprise'}\n",
    "drawing_spec = mp_drawings.DrawingSpec(thickness=1, circle_radius=1)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=2,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        success, annotated_image = cap.read()\n",
    "        height,width,_= annotated_image.shape\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        annotated_image.flags.writeable = False\n",
    "        annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(annotated_image)\n",
    "\n",
    "        # Draw the face mesh annotations on the image.\n",
    "        annotated_image.flags.writeable = True\n",
    "        annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                data = []\n",
    "                for i in DISTANCES:\n",
    "                    x1=int(face_landmarks.landmark[i[0]].x * width)\n",
    "                    y1=int(face_landmarks.landmark[i[0]].y * height)\n",
    "                    x2=int(face_landmarks.landmark[i[1]].x * width)\n",
    "                    y2=int(face_landmarks.landmark[i[1]].y * height)\n",
    "                    # cv2.line(img=annotated_image,pt1=[x1,y1],pt2=[x2,y2],color=(0,0,255),thickness=1)\n",
    "                    data.append(calculate_distance(face_landmarks.landmark[i[0]],face_landmarks.landmark[i[1]]))\n",
    "                prediction = loaded_model.predict(np.array(data).reshape((1, -1)))[0]\n",
    "                emotion_image = cv2.imread(os.path.join('../DATA/Assests/',emotions[prediction]+'.png'))\n",
    "                emotion_image_size = 50\n",
    "                emotion_image = cv2.resize(emotion_image, (emotion_image_size, emotion_image_size))\n",
    "                emotion_image_gray = cv2.cvtColor(emotion_image, cv2.COLOR_BGR2GRAY)\n",
    "                ret, mask = cv2.threshold(emotion_image_gray, 1, 255, cv2.THRESH_BINARY)\n",
    "                roi = annotated_image[-emotion_image_size-10:-10, -emotion_image_size-10:-10]\n",
    "\n",
    "                roi[np.where(mask)] = 0\n",
    "                roi += emotion_image\n",
    "                cv2.putText(img=annotated_image,text=emotions[prediction],org=(int(face_landmarks.landmark[UPPER_REF].x * width)-20,int(face_landmarks.landmark[UPPER_REF].y * width)-30),fontFace=cv2.FONT_HERSHEY_DUPLEX,fontScale=1,color=(255,0,0),thickness=1,)\n",
    "\n",
    "        cv2.imshow('MediaPipe Face Mesh', annotated_image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}